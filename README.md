# Neural-Networks-Activation-Functions-Optimizers-and-Regularization
Mini project to experiment with neural network architecture and evaluate how different activation functions, optimizers, and regularization techniques impact model accuracy. 

I. Use Pytorch to create a neural network classifier and experiment with the following architecture: 
* Activation functions:
  1. Rectified Linear Units (ReLu) Activation Function
  2. Identify Activation Function
  3. Sigmoid Activation Function
 
* Optimizers:
  1. Baeline - no optimizer (sigmoid activation function)
  2. Stochastic Gradient Descent (SGD) Optimizer 
  3. Adam Optimizer
  4. Adagrad Optimizer

* Regularizations
  1. Baseline (no regularization)
  2. Batch Norm Regularization
  3. Dropout Regularization
  4. Weight Init Regularization
 
II. Results 

![Screenshot 2025-07-08 at 11 05 25 AM](https://github.com/user-attachments/assets/93c6bb40-2ae4-4cc3-aeff-e582b3b3cd22)


![Screenshot 2025-07-08 at 11 05 38 AM](https://github.com/user-attachments/assets/52ed78d9-2abf-4482-b9a5-790f79525532)



![Screenshot 2025-07-08 at 11 05 51 AM](https://github.com/user-attachments/assets/d0461f72-6b3b-4b78-bf40-4eebf252f1db)
